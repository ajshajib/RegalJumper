{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Authors:** Anowar Shajib, David Law\n",
    "\n",
    "*Adapted from the [notebook](https://github.com/JWST-Templates/Notebooks/blob/main/MIRI_MRS_reduction_SPT0418-47_PAH_ch3long.ipynb) by the TEMPLATES team (J. Spilker, K. A. Phadke, D. Law).*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24fab7ed739ebc36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.ndimage import median_filter\n",
    "import pathlib\n",
    "\n",
    "from util import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aab7a45bd348bd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_pipeline = False\n",
    "do_extra_processing = True\n",
    "rate_file_directory_for_processing = stage1_directory\n",
    "\n",
    "msa_leakage_file_ids = [\"10101\", \"12101\", \"14101\", \"16101\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1432c1c01ddb544"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STAGE 1 DETECTOR PIPELINE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "467f58916e80a6df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to call the detector1 pipeline with desired parameters.\n",
    "# Individual steps not enumerated. Options can be set as commented overrides.\n",
    "\n",
    "\n",
    "def run_spec1_pipeline(file_name, output_directory):\n",
    "    \"\"\"\n",
    "    Create a Detector1Pipeline object and set all the desired stage 1\n",
    "    pipeline processing arguments.\n",
    "\n",
    "    :param file_name: _uncal.fits exposure file name\n",
    "    :param output_directory: Directory for the stage 1 output files\n",
    "    :return: None\n",
    "    :outputs: writes  _rate.fits and _rateints.fits files in `outdir`\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running Detector1Pipeline on {0:s}...\".format(file_name))\n",
    "\n",
    "    crds_config = Detector1Pipeline.get_config_from_reference(file_name)\n",
    "    detector_1_pipeline = Detector1Pipeline.from_config_section(crds_config)\n",
    "    detector_1_pipeline.output_dir = output_directory\n",
    "\n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    # detector_1_pipeline.dq_init.skip = False\n",
    "    # detector_1_pipeline.saturation.skip = False\n",
    "    # detector_1_pipeline.firstframe.skip = False\n",
    "    # detector_1_pipeline.lastframe.skip = False\n",
    "    # detector_1_pipeline.reset.skip = False\n",
    "    # detector_1_pipeline.linearity.skip = False\n",
    "    # detector_1_pipeline.rscd.skip = False\n",
    "    # detector_1_pipeline.dark_current.skip = False\n",
    "    # detector_1_pipeline.refpix.skip = False\n",
    "    # detector_1_pipeline.jump.skip = False\n",
    "    # detector_1_pipeline.ramp_fit.skip = False\n",
    "    # detector_1_pipeline.gain_scale.skip = False\n",
    "\n",
    "    # Trun on multiprocessing for jump and ramp fitting steps\n",
    "    detector_1_pipeline.jump.maximum_cores = \"half\"\n",
    "    detector_1_pipeline.ramp_fit.maximum_cores = \"half\"\n",
    "\n",
    "    # S_DARK  = 'COMPLETE'           / Dark Subtraction\n",
    "    # S_DQINIT= 'COMPLETE'           / Data Quality Initialization\n",
    "    # S_GANSCL= 'COMPLETE'           / Gain Scale Correction\n",
    "    # S_GRPSCL= 'COMPLETE'           / Group Scale Correction\n",
    "    # S_IPC   = 'SKIPPED '           / Interpixel Capacitance Correction\n",
    "    # S_JUMP  = 'COMPLETE'           / Jump Detection\n",
    "    # S_LINEAR= 'COMPLETE'           / Linearity Correction\n",
    "    # S_RAMP  = 'COMPLETE'           / Ramp Fitting\n",
    "    # S_REFPIX= 'COMPLETE'           / Reference Pixel Correction\n",
    "    # S_SATURA= 'COMPLETE'           / Saturation Checking\n",
    "    # S_SUPERB= 'COMPLETE'           / Superbias Subtraction\n",
    "\n",
    "    # Bad pixel mask overrides\n",
    "    # detector_1_pipeline.dq_init.override_mask = 'myfile.fits'\n",
    "\n",
    "    # Saturation overrides\n",
    "    # et1.saturation.override_saturation = 'myfile.fits'\n",
    "\n",
    "    # Reset overrides\n",
    "    # detector_1_pipeline.reset.override_reset = 'myfile.fits'\n",
    "\n",
    "    # Linearity overrides\n",
    "    # detector_1_pipeline.linearity.override_linearity = 'myfile.fits'\n",
    "\n",
    "    # RSCD overrides\n",
    "    # detector_1_pipeline.rscd.override_rscd = 'myfile.fits'\n",
    "\n",
    "    # DARK overrides\n",
    "    # detector_1_pipeline.dark_current.override_dark = 'myfile.fits'\n",
    "\n",
    "    # GAIN overrides\n",
    "    # detector_1_pipeline.jump.override_gain = 'myfile.fits'\n",
    "    # detector_1_pipeline.ramp_fit.override_gain = 'myfile.fits'\n",
    "\n",
    "    # READNOISE overrides\n",
    "    # detector_1_pipeline.jump.override_readnoise = 'myfile.fits'\n",
    "    # detector_1_pipeline.ramp_fit.override_readnoise = 'myfile.fits'\n",
    "\n",
    "    # JUMP overrides.\n",
    "    # Currently pipeline is not flagging lower-level jumps\n",
    "    # like we might want it to, so lower thresholds for more\n",
    "    # aggressive flagging.\n",
    "    # detector_1_pipeline.jump.save_results = True\n",
    "    # detector_1_pipeline.jump.rejection_threshold = 3.5  # default 4.0sigma\n",
    "    # detector_1_pipeline.jump.min_jump_to_flag_neighbors = 8.0  # default 10sigma\n",
    "\n",
    "    # Additional JUMP overrides related to CR shower flagging. See\n",
    "    # JWST pipeline documentation page for details of parameters.\n",
    "    # https://jwst-pipeline.readthedocs.io/en/stable/jwst/jump/index.html\n",
    "    detector_1_pipeline.jump.expand_large_events = True  # Turn on shower flagging\n",
    "    # detector_1_pipeline.jump.use_ellipses = True  # approximate showers as elliptical, obsolete in newest pipeline\n",
    "    # detector_1_pipeline.jump.min_jump_area = 12  # Min # of contiguous pixels to trigger expanded flagging\n",
    "    # detector_1_pipeline.jump.sat_required_snowball = False  # Require pixels to be saturated to flag\n",
    "    # detector_1_pipeline.jump.expand_factor = 3.0  # expands showers beyond ID'd jumps; default 2.0\n",
    "    #\n",
    "    # detector_1_pipeline.jump.after_jump_flag_dn1 = 10  # These 4 related to how long after a jump is identified\n",
    "    # detector_1_pipeline.jump.after_jump_flag_time1 = 20  #  we should keep flagging the following integrations\n",
    "    # detector_1_pipeline.jump.after_jump_flag_dn2 = 1000\n",
    "    # detector_1_pipeline.jump.after_jump_flag_time2 = 3000\n",
    "\n",
    "    ### settings from the original TEMPLATES notebook used for MIRI, for reference\n",
    "    # # JUMP overrides.\n",
    "    # # Currently pipeline is not flagging lower-level jumps\n",
    "    # # like we might want it to, so lower thresholds for more\n",
    "    # # aggressive flagging.\n",
    "    # #detector_1_pipeline.jump.save_results = True\n",
    "    # detector_1_pipeline.jump.rejection_threshold = 3.5         # default 4.0sigma\n",
    "    # detector_1_pipeline.jump.min_jump_to_flag_neighbors = 8.0  # default 10sigma\n",
    "    #\n",
    "    # # Additional JUMP overrides related to CR shower flagging. See\n",
    "    # # JWST pipeline documentation page for details of parameters.\n",
    "    # # https://jwst-pipeline.readthedocs.io/en/stable/jwst/jump/index.html\n",
    "    # detector_1_pipeline.jump.expand_large_events   = True   # Turn on shower flagging\n",
    "    # detector_1_pipeline.jump.use_ellipses          = True   # True for MIRI; approximate showers as elliptical\n",
    "    # detector_1_pipeline.jump.min_jump_area         = 8     # Min # of contiguous pixels to trigger expanded flagging\n",
    "    # # The saturated core allows the search for smaller events without false positives.\n",
    "    # detector_1_pipeline.jump.sat_required_snowball = False  # Do not require pixels to be saturated to flag\n",
    "    # detector_1_pipeline.jump.expand_factor         = 3.0    # expands showers beyond ID'd jumps; default 2.0\n",
    "    #\n",
    "    # detector_1_pipeline.jump.after_jump_flag_dn1   = 10     # These 4 related to how long after a jump is identified\n",
    "    # detector_1_pipeline.jump.after_jump_flag_time1 = 20     #  we should keep flagging the following integrations\n",
    "    # detector_1_pipeline.jump.after_jump_flag_dn2   = 1000\n",
    "    # detector_1_pipeline.jump.after_jump_flag_time2 = 3000\n",
    "    ### TEMPLATES settings end\n",
    "\n",
    "    # Ramp_fit overrides.\n",
    "    # detector_1_pipeline.ramp_fit.save_opt = True\n",
    "    # detector_1_pipeline.ramp_fit.save_results = True\n",
    "\n",
    "    detector_1_pipeline.save_results = True  # Save the final resulting _rate.fits files\n",
    "\n",
    "    detector_1_pipeline(file_name)  # Run the pipeline on an input list of files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "102b7a3509f0fbe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grab a list of all uncal files from the folder\n",
    "files_uncal = sorted(glob.glob(uncal_directory + \"*nrs1_uncal.fits\"))\n",
    "\n",
    "# Run the Detector1 pipeline on each uncal exposure.\n",
    "if run_pipeline:\n",
    "    for file in tqdm(files_uncal):\n",
    "        run_spec1_pipeline(file_name=file, output_directory=stage1_directory)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5aa5be9346def8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extra processing to subtract the leak images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4746d0175928a34c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if do_extra_processing:\n",
    "    # Make a median leak image\n",
    "    all_files = np.array(sorted(glob.glob(rate_file_directory_for_processing + \"*nrs1_rate.fits\")))\n",
    "    leak_files = []\n",
    "    for file in all_files:\n",
    "        for msa_leakage_file_id in msa_leakage_file_ids:\n",
    "            if msa_leakage_file_id in file:\n",
    "                leak_files.append(file)\n",
    "                break\n",
    "\n",
    "    # Read in all the leak files\n",
    "    big_array = np.zeros([len(leak_files), 2048, 2048])\n",
    "    for i, leak_file in enumerate(leak_files):\n",
    "        with fits.open(leak_file) as hdu:\n",
    "            big_array[i, :, :] = hdu[\"SCI\"].data\n",
    "\n",
    "    median_leak = np.nanmedian(big_array, axis=0)\n",
    "\n",
    "    # Clean up the leak image\n",
    "    leak2 = median_leak.copy()\n",
    "\n",
    "    # First mask anything with counts > 0.5 DN/s\n",
    "    leak2[median_leak > 0.5] = np.nanmedian(leak2)\n",
    "    \n",
    "    # Then deal with NaNs\n",
    "    leak2[np.isnan(leak2)] = np.nanmedian(leak2)\n",
    "\n",
    "    # Then apply a horizontal median filter to take out RMS noise\n",
    "    leak2 = median_filter(leak2, size=(1, 51))\n",
    "\n",
    "    # Subtract leak image from science files\n",
    "    sci_files = []\n",
    "    for file in all_files:\n",
    "        if file not in leak_files:\n",
    "            sci_files.append(file)\n",
    "\n",
    "    for sci_file in sci_files:\n",
    "        with fits.open(sci_file) as hdu:\n",
    "            sci = hdu[\"SCI\"].data\n",
    "            sci = sci - leak2\n",
    "            hdu[\"SCI\"].data = sci\n",
    "            name = pathlib.Path(sci_file).name\n",
    "            hdu.writeto(stage1_processed_directory + name, overwrite=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f7bbe53deb7d1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extra processing to flag bad pixels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484104e5a9edd467"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if do_extra_processing:\n",
    "    files = np.array(sorted(glob.glob(stage1_processed_directory + \"*1001*nrs1_rate.fits\")))\n",
    "\n",
    "    for file in files:\n",
    "        with fits.open(file) as hdu:\n",
    "            sci = hdu[\"SCI\"].data\n",
    "\n",
    "            # Flag positive pixels\n",
    "            temp = sci.copy()\n",
    "            temp[np.isfinite(temp) != True] = 0.0\n",
    "            sci2 = median_filter(temp, size=(1, 11))\n",
    "            diff = temp - sci2 * 2\n",
    "            index = np.where(diff > 0.5)\n",
    "            sci[index] = np.nan\n",
    "            # Grow by 1 pixel in both directions\n",
    "            y_index = index[1]\n",
    "            x_index = index[0]\n",
    "            n_index = len(y_index)\n",
    "\n",
    "            for i in range(n_index):\n",
    "                sci[x_index[i] - 1, y_index[i]] = np.nan\n",
    "                sci[x_index[i] + 1, y_index[i]] = np.nan\n",
    "                sci[x_index[i], y_index[i] - 1] = np.nan\n",
    "                sci[x_index[i], y_index[i] + 1] = np.nan\n",
    "\n",
    "            # Flag negative pixels\n",
    "            temp = sci.copy()\n",
    "            temp[np.isfinite(temp) != True] = 0.0\n",
    "            sci2 = median_filter(temp, size=(1, 11))\n",
    "            diff = temp - sci2 * 0.5\n",
    "            index = np.where(diff < -0.5)\n",
    "            sci[index] = np.nan\n",
    "            # Grow by 1 pixel in both directions\n",
    "            y_index = index[1]\n",
    "            x_index = index[0]\n",
    "            n_index = len(y_index)\n",
    "            for i in range(n_index):\n",
    "                sci[x_index[i] - 1, y_index[i]] = np.nan\n",
    "                sci[x_index[i] + 1, y_index[i]] = np.nan\n",
    "                sci[x_index[i], y_index[i] - 1] = np.nan\n",
    "                sci[x_index[i], y_index[i] + 1] = np.nan\n",
    "\n",
    "            hdu[\"SCI\"].data = sci\n",
    "            hdu.writeto(file, overwrite=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f433e8a4b34f9e45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cd9d9c9bb88ab5d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
