{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f171f85c02dbd9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Author:** Anowar Shajib\n",
    "\n",
    "(Some of the decription below adapted from [STScI notebooks](https://github.com/spacetelescope/jwst-pipeline-notebooks/tree/main).\n",
    "\n",
    "In this notebook, we process our data through the Spec3 pipeline to produce Lvl3 data products, including calibrated slope images, quick-look data cubes, and 1D spectra.\n",
    "\n",
    "For more details, see the [Spec3 pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html).\n",
    "\n",
    "Here, we'll run the Spec3 pipeline to produce a composite data cube from all dithered exposures. We will need to create an association file from all science and background data for the pipeline to use them appropriately.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "A word of caution: the data cubes created by the JWST pipeline are in SURFACE BRIGHTNESS units (MJy/steradian), not flux units.  What that means is that if you intend to sum spectra within an aperture you need to be sure to multiply by the pixel area in steradians first in order to get a spectrum in flux units (the PIXAR_SR keyword can be found in the SCI extension header).  This correction is already build into the pipeline Extract1D algorithm.\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html    \n",
    "</div>\n",
    "\n",
    "Spec3 requires an association file to be created to associate the individual _cal.fits science exposures together to be combined into a final combined data cube.\n",
    "\n",
    "- If Master background subtraction was chosen, this will be applied during Spec3. Doing the Background subtraction requires the association file to be created to associate the background _x1d.fits files with all science files. For Master Background subtraction the association file must contain all _x1d.fits background files created with Spec2 and all _cal.fits science files to be combined into the final background subtracted science cube. \n",
    "\n",
    "The extract_1d step is controlled by a different set of parameters in the EXTRACT1D reference file.\n",
    "\n",
    "For point sources:\n",
    ">For point sources a circular extraction aperture is used, along with an optional circular annulus for background extraction and subtraction. The size of the extraction region and the background annulus size varies with wavelength. The extraction related vectors are found in the asdf extract1d reference file. [More Info ...](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html)\n",
    "\n",
    "For extended targets:\n",
    "> For an extended source, rectangular aperture photometry is used, with the entire image being extracted, and no background subtraction, regardless of what was specified in the reference file or step arguments. [More Info ...](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27fba6dec1f9d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_pipeline = True\n",
    "subtract_combined_background = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1140d540fb6d46a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_association_file(\n",
    "    science_files, background_files, association_file, product_name=\"Level3\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Write a .json association file that specifies a list of\n",
    "    science and background exposures to be processed by Spec3.\n",
    "    Note that any background exposures have to be of type x1d.\n",
    "\n",
    "    :param science_files: List of science exposures to be processed\n",
    "    :param association_file: Output .json file path/name\n",
    "    :param product_name: AFAIK should just be 'Level3' always but prob\n",
    "                      doesn't actually affect anything but metadata.\n",
    "    :return: None\n",
    "    :outputs: Writes a .json association file to the location in `asn_file'\n",
    "    \"\"\"\n",
    "    # Define the basic association of science files\n",
    "    association = asn_from_list(\n",
    "        science_files, rule=DMS_Level3_Base, product_name=product_name\n",
    "    )\n",
    "\n",
    "    # Add background files to the association\n",
    "    for background_file in background_files:\n",
    "        association[\"products\"][0][\"members\"].append(\n",
    "            {\"expname\": background_file, \"exptype\": \"background\"}\n",
    "        )\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = association.dump()\n",
    "    with open(association_file, \"w\") as outfile:\n",
    "        outfile.write(serialized)\n",
    "\n",
    "\n",
    "def run_spec3_pipeline(association_file, output_directory, output_file=None):\n",
    "    \"\"\"\n",
    "    Run the Spec3 pipeline using the given association file. The association\n",
    "    file must contain the information needed - sci and bg cal files - to\n",
    "    run the pipeline.\n",
    "\n",
    "    :param association_file: file to run through the pipeline\n",
    "    :param output_directory: Directory for the stage3 output files\n",
    "    :return: None\n",
    "    :outputs: Writes _cal.fits files for each exposure to `output_directory'. Also saves\n",
    "              median-combined background images to `output_directory'. Additionally if\n",
    "              nocubes=False, writes a 3D data cube built from each exposure\n",
    "              to `output_directory'.\n",
    "    \"\"\"\n",
    "\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(association_file)\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "\n",
    "    spec3.output_dir = output_directory\n",
    "    spec3.save_results = True\n",
    "\n",
    "    # Background overrides were set up above\n",
    "    spec3.master_background.skip = True\n",
    "\n",
    "    # Overrides for whether or not certain other steps should be skipped\n",
    "    # spec3.assign_mtwcs.skip = False\n",
    "    spec3.outlier_detection.skip = False\n",
    "    # spec3.outlier_detection.kernel_size = '11 1'\n",
    "    # spec3.outlier_detection.threshold_percent = 99.5 # Dial this threshold if necessary to be more/less aggressive in outlier flagging\n",
    "    # spec3.mrs_imatch.skip = True\n",
    "    # spec3.cube_build.skip = True\n",
    "    # spec3.extract_1d.skip = False\n",
    "\n",
    "    # Cube building configuration options\n",
    "    # spec3.cube_build.override_cubepar = 'myfile.fits' # Cubepar parameter override\n",
    "    if output_file is not None:\n",
    "        spec3.cube_build.output_file = output_file  # Custom output name\n",
    "    # spec3.cube_build.output_type = 'band' # 'band', 'channel', or 'multi' type cube output\n",
    "    # spec3.cube_build.channel = '1' # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "    spec3.cube_build.weighting = \"drizzle\"  # 'emsm' or 'drizzle'\n",
    "    spec3.cube_build.coord_system = (\n",
    "        \"ifualign\"  # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "    )\n",
    "    spec3.cube_build.scalexy = (\n",
    "        0.05  # Output cube spaxel scale (arcsec) if setting it by hand\n",
    "    )\n",
    "    # spec3.cube_build.scalew = 0.002 # Output cube voxel depth in wavelength if setting it by hand\n",
    "    # spec3.cube_build.ra_center = 65.0 # Force cube to be centered at this R.A.\n",
    "    # spec3.cube_build.dec_center = -35.0 # Force cube to be centered at this Decl.\n",
    "    # spec3.cube_build.cube_pa = 45.0 # Force cube to have this position angle\n",
    "    # spec3.cube_build.nspax_x = 61 # Require this number of spaxels in cube X direction\n",
    "    # spec3.cube_build.nspax_y = 61 # Require this number of spaxels in cube Y direction\n",
    "    # spec3.cube_build.wavemin = 4.8 # Custom minimum wavelength for the cube\n",
    "    # spec3.cube_build.wavemax = 1.6  # Custom maximum wavelength for the cube\n",
    "\n",
    "    # Extract1D overrides and config options\n",
    "    # spec3.extract_1d.override_extract1d = 'myfile.asdf'\n",
    "    # spec3.extract_1d.override_apcorr = 'myfile.asdf'\n",
    "    # spec3.extract_1d.ifu_set_srctype = 'POINT' # Force a certain type of spectral extraction\n",
    "    # spec3.extract_1d.ifu_rscale = 2 # Number of FWHM to use for point-source aperture extraction radius (default is 2)\n",
    "    # spec3.extract_1d.ifu_autocen = True # Enable auto-centering of the extraction aperture\n",
    "    # spec3.extract_1d.center_xy=(20,20) # Override aperture location if desired\n",
    "    # spec3.extract_1d.ifu_rfcorr = True # Turn on 1d residual fringe correction\n",
    "\n",
    "    spec3.save_results = True\n",
    "    spec3(association_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c183093446c83c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab our lists of sci & background exposures.\n",
    "# Note that since we did 2D bkg subtraction earlier we don't actually\n",
    "# need these 1D backgrounds (they will be skipped by the Spec3 pipeline),\n",
    "# but if you modified Spec2 to *not* do 2D subtraction, this would\n",
    "# be how you would use them for a 1D master background subtraction here.\n",
    "all_files = sorted(glob.glob(stage2_processed_directory + \"jw*cal.fits\"))\n",
    "\n",
    "science_files = all_files[:16]\n",
    "background_files = all_files[16:]\n",
    "\n",
    "science_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3b9bb0f5c0eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures\n",
    "association_file = os.path.join(stage3_directory, \"level3_asn.json\")\n",
    "\n",
    "write_to_association_file(science_files, background_files, association_file, \"Level3\")\n",
    "\n",
    "if run_pipeline:\n",
    "    run_spec3_pipeline(association_file, stage3_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05736cf5c137ec7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## make cube for the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf56e3bf8ffdd5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "association_file = os.path.join(stage3_directory, \"level3_background_asn.json\")\n",
    "\n",
    "for i in range(len(background_files)):\n",
    "    write_to_association_file(\n",
    "        [background_files[i]],\n",
    "        [background_files[i]],\n",
    "        association_file,\n",
    "        \"Level3\",\n",
    "    )\n",
    "\n",
    "    if run_pipeline:\n",
    "        run_spec3_pipeline(\n",
    "            association_file, stage3_directory, output_file=f\"background_{i}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91523e276909055a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c3c890a50f2dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code is adapted from the TEMPLATE team's cookbook\n",
    "# https://github.com/JWST-Templates/Notebooks/blob/main/nirspec_ifu_cookbook.ipynb\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def subtract_background(cubefile, background_file, outfile=None):\n",
    "    # first, load the expected background data:\n",
    "    background_sim = np.loadtxt(background_file)\n",
    "    background_lambda = np.array([line[0] for line in background_sim])\n",
    "    background_flux = np.array([line[1] for line in background_sim])\n",
    "\n",
    "    # and make an interp1d object sdo we can get the background at any given point:\n",
    "    background_interp = interp1d(background_lambda, background_flux)\n",
    "    # now load the data cube:\n",
    "\n",
    "    with fits.open(cubefile) as hdu:\n",
    "        data = hdu[1].data\n",
    "        header = hdu[1].header\n",
    "        cube_wavlength = np.arange(\n",
    "            header[\"CRVAL3\"],\n",
    "            header[\"CRVAL3\"] + (header[\"CDELT3\"] * len(data)),\n",
    "            header[\"CDELT3\"],\n",
    "        )\n",
    "        if len(cube_wavlength) > len(data):\n",
    "            cube_wavlength = cube_wavlength[:-1]\n",
    "        # next, evaluate background on data wavelength points:\n",
    "        bgcalculated = background_interp(cube_wavlength)\n",
    "        bgcube = np.tile(bgcalculated, (data.shape[2], data.shape[1], 1)).T  # cubeify!\n",
    "        # and subtract!\n",
    "        # print(len(data), len(cubewl))\n",
    "        background_subtracted_cube = data - bgcube\n",
    "        # save or return result:\n",
    "        if outfile:\n",
    "            hdu[1].data = background_subtracted_cube\n",
    "            hdu[1].header[\"comment\"] = \"Expected Background Subtracted\"\n",
    "            hdu.writeto(outfile, overwrite=True)\n",
    "        else:\n",
    "            return background_subtracted_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubefile = stage3_directory + \"Level3_g140m-f100lp_s3d.fits\"\n",
    "background_file = \"../background_sim/RXJ1131_bkgd.txt\"\n",
    "\n",
    "subtract_background(\n",
    "    cubefile,\n",
    "    background_file,\n",
    "    outfile=stage3_directory + \"Level3_g140m-f100lp_s3d_bkgsub.fits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433be2ec27404e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot the spectra\n",
    "\n",
    "Here we'll plot the spectra to see what our source looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2445ed895fc8320",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, header = fits.getdata(\n",
    "    stage3_directory + \"Level3_g140m-f100lp_s3d_bkgsub.fits\",\n",
    "    header=True,\n",
    ")\n",
    "\n",
    "data2, header = fits.getdata(\n",
    "    stage3_directory + \"Level3_g140m-f100lp_s3d_bkgsub_earlier.fits\",\n",
    "    header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4971725f4cac8b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.matshow(np.log10(np.nansum(data, axis=0)))\n",
    "plt.title(\"Summed datacube\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "x_pix, y_pix = 53, 60\n",
    "ymin, ymax = np.nan, np.nan\n",
    "\n",
    "wavelength = header[\"CRVAL3\"] + header[\"CDELT3\"] * np.arange(header[\"NAXIS3\"])\n",
    "flux = data[:, y_pix, x_pix]\n",
    "\n",
    "ymin = np.nanmin([ymin, np.nanpercentile(flux, 2)])\n",
    "ymax = np.nanmax([ymax, np.nanpercentile(flux, 99.8)])\n",
    "\n",
    "plt.plot(wavelength, flux)\n",
    "\n",
    "plt.xlabel(r\"Wavelength ($\\mu$m)\")\n",
    "plt.ylabel(f\"Flux ({header['BUNIT']})\")\n",
    "plt.title(f\"pixel: {x_pix}, {y_pix}\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe39633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
